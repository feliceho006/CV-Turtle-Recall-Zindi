{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Efficientnet + ArcFace [training]\n",
    "## Introduction\n",
    "* This notebook uses Efficient net and ArcFace on pytorch for training a model.\n",
    "* to train for 10 epoch I got validation accuracy about 0.25 and validation GAP score about 0.20\n",
    "* Inference notebook for submission is another one https://www.kaggle.com/zaccheroni/pytorch-efficientnet-arcface-submission Here!\n",
    "* To tell the truth I'm a beginer, so if there are any mistakes, please tell me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a notebook https://www.kaggle.com/rhtsingh/pytorch-training-inference-efficientnet-baseline by @rhtsingh - as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:35.138296Z",
     "iopub.status.busy": "2022-04-17T18:35:35.137918Z",
     "iopub.status.idle": "2022-04-17T18:35:45.823709Z",
     "shell.execute_reply": "2022-04-17T18:35:45.822660Z",
     "shell.execute_reply.started": "2022-04-17T18:35:35.138266Z"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /tmp/pip/cache/\n",
    "# !cp ../input/resources-for-google-landmark-recognition-2020/efficientnet_pytorch-0.6.3-py3-none-any.whl /tmp/pip/cache/\n",
    "# !pip install --no-index --find-links /tmp/pip/cache/ efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:45.828097Z",
     "iopub.status.busy": "2022-04-17T18:35:45.827677Z",
     "iopub.status.idle": "2022-04-17T18:35:47.194175Z",
     "shell.execute_reply": "2022-04-17T18:35:47.193153Z",
     "shell.execute_reply.started": "2022-04-17T18:35:45.828046Z"
    }
   },
   "outputs": [],
   "source": [
    "# import efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:47.197262Z",
     "iopub.status.busy": "2022-04-17T18:35:47.196960Z",
     "iopub.status.idle": "2022-04-17T18:35:47.203275Z",
     "shell.execute_reply": "2022-04-17T18:35:47.200845Z",
     "shell.execute_reply.started": "2022-04-17T18:35:47.197230Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you need notification\n",
    "#!pip install slackweb\n",
    "# import slackweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:47.205361Z",
     "iopub.status.busy": "2022-04-17T18:35:47.204800Z",
     "iopub.status.idle": "2022-04-17T18:35:49.484114Z",
     "shell.execute_reply": "2022-04-17T18:35:49.483214Z",
     "shell.execute_reply.started": "2022-04-17T18:35:47.205295Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:49.490655Z",
     "iopub.status.busy": "2022-04-17T18:35:49.490256Z",
     "iopub.status.idle": "2022-04-17T18:35:49.598208Z",
     "shell.execute_reply": "2022-04-17T18:35:49.597271Z",
     "shell.execute_reply.started": "2022-04-17T18:35:49.490608Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from PIL import Image  # Image utilities.\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import imageio as io_temp\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:35:49.603506Z",
     "iopub.status.busy": "2022-04-17T18:35:49.603160Z",
     "iopub.status.idle": "2022-04-17T18:35:49.612499Z",
     "shell.execute_reply": "2022-04-17T18:35:49.611232Z",
     "shell.execute_reply.started": "2022-04-17T18:35:49.603472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed everything to avoid non-determinism.\n",
    "def seed_everything(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:37:58.140625Z",
     "iopub.status.busy": "2022-04-17T18:37:58.140219Z"
    }
   },
   "outputs": [],
   "source": [
    "SOURCE_URL = 'https://storage.googleapis.com/dm-turtle-recall/images.tar'\n",
    "IMAGE_DIR = './turtle_recall/images'\n",
    "TAR_PATH = os.path.join(IMAGE_DIR, os.path.basename(SOURCE_URL))\n",
    "EXPECTED_IMAGE_COUNT = 13891\n",
    "\n",
    "%sx mkdir --parents \"{IMAGE_DIR}\"\n",
    "if len(os.listdir(IMAGE_DIR)) != EXPECTED_IMAGE_COUNT:\n",
    "  %sx wget --no-check-certificate -O \"{TAR_PATH}\" \"{SOURCE_URL}\"\n",
    "  %sx tar --extract --file=\"{TAR_PATH}\" --directory=\"{IMAGE_DIR}\"\n",
    "  %sx rm \"{TAR_PATH}\"\n",
    "\n",
    "print(f'The total number of images is: {len(os.listdir(IMAGE_DIR))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:37:41.418109Z",
     "iopub.status.busy": "2022-04-17T18:37:41.416851Z",
     "iopub.status.idle": "2022-04-17T18:37:41.438236Z",
     "shell.execute_reply": "2022-04-17T18:37:41.436872Z",
     "shell.execute_reply.started": "2022-04-17T18:37:41.418045Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T18:37:10.822345Z",
     "iopub.status.busy": "2022-04-17T18:37:10.821989Z",
     "iopub.status.idle": "2022-04-17T18:37:30.900283Z",
     "shell.execute_reply": "2022-04-17T18:37:30.896783Z",
     "shell.execute_reply.started": "2022-04-17T18:37:10.822314Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://storage.googleapis.com/dm-turtle-recall/'\n",
    "\n",
    "\n",
    "def read_csv_from_web(file_name):\n",
    "  url = urllib.parse.urljoin(BASE_URL, file_name)\n",
    "  content = requests.get(url).content\n",
    "  return pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
    "\n",
    "# Read in csv files.\n",
    "# train = read_csv_from_web('train.csv')\n",
    "test = read_csv_from_web('test.csv')\n",
    "# extra = read_csv_from_web('extra_images.csv')\n",
    "# sample_submission = read_csv_from_web('sample_submission.csv')\n",
    "\n",
    "# Convert image_location strings to lowercase.\n",
    "for df in [test]:\n",
    "  df.image_location = df.image_location.apply(lambda x: x.lower())\n",
    "  assert set(df.image_location.unique()) == set(['left', 'right', 'top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.972496Z",
     "iopub.status.idle": "2022-04-17T18:36:29.973922Z"
    }
   },
   "outputs": [],
   "source": [
    "# ls = pd.unique(train['turtle_id'])\n",
    "# classes = {}\n",
    "# for i in range(len(ls)):\n",
    "#     classes[ls[i]] = i\n",
    "# class_names = classes\n",
    "# num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.975454Z",
     "iopub.status.idle": "2022-04-17T18:36:29.976459Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(train[['image_id','image_location']], train['turtle_id'], test_size=0.20, random_state=42, stratify=train['turtle_id'])\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50, random_state=42, stratify=y_test)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, X_val.shape ,y_train.shape, y_test.shape, y_val.shape)\n",
    "\n",
    "# train_ds = pd.concat([X_train, y_train], axis=1)\n",
    "# train_ds['type'] = \"train\"\n",
    "# test_ds = pd.concat([X_test, y_test], axis=1)\n",
    "# test_ds['type'] = \"test\"\n",
    "# val_ds = pd.concat([X_val, y_val], axis=1)\n",
    "# val_ds['type'] = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.977994Z",
     "iopub.status.idle": "2022-04-17T18:36:29.978951Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_sizes = {'train': len(train) + len(extra)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.980709Z",
     "iopub.status.idle": "2022-04-17T18:36:29.981745Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([train,extra], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.983391Z",
     "iopub.status.idle": "2022-04-17T18:36:29.984444Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['image_id'] = IMAGE_DIR + \"/\" + dataset['image_id'].astype(str) + \".JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.985982Z",
     "iopub.status.idle": "2022-04-17T18:36:29.987068Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['turtle_id'].isin(train['turtle_id'])]\n",
    "dataset = dataset.reset_index()\n",
    "dataset.pop('index')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.988604Z",
     "iopub.status.idle": "2022-04-17T18:36:29.989624Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset['turtle_id'] = dataset['turtle_id'].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.991242Z",
     "iopub.status.idle": "2022-04-17T18:36:29.992205Z"
    }
   },
   "outputs": [],
   "source": [
    "class TurtleDataset(Dataset):\n",
    "    \"\"\"Turtle dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.csv = csv_file\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.csv.iloc[idx, 0]\n",
    "        image = io_temp.imread(img_name)\n",
    "        details = self.csv.iloc[idx, 1:]\n",
    "        details = np.array([details])\n",
    "        details = details[:1]\n",
    "        sample = {'image': image, 'image_orientation': details[0][0], 'turtle_id': details[0][1]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.993643Z",
     "iopub.status.idle": "2022-04-17T18:36:29.994633Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size = 224):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, turtle_id = sample['image'], sample['turtle_id'] # , sample['image_orientation']\n",
    "        \n",
    "        # ADD CODE EITHER HERE\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        crop_size = min(w, h)\n",
    "        crop = image[(h - crop_size) // 2 : (h + crop_size) // 2, (w - crop_size) // 2 : (w + crop_size) // 2]\n",
    "        img = resize(crop, (self.output_size, self.output_size))\n",
    "\n",
    "        # OR ADD CODE HERE\n",
    "        \n",
    "        return [img.transpose((2,0,1)).astype(np.double),turtle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.996202Z",
     "iopub.status.idle": "2022-04-17T18:36:29.997325Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale_test(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size = 224):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, turtle_id = sample['image'], sample['turtle_id'] # , sample['image_orientation']\n",
    "        \n",
    "        # ADD CODE EITHER HERE\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        crop_size = min(w, h)\n",
    "        crop = image[(h - crop_size) // 2 : (h + crop_size) // 2, (w - crop_size) // 2 : (w + crop_size) // 2]\n",
    "        img = resize(crop, (self.output_size, self.output_size))\n",
    "\n",
    "        # OR ADD CODE HERE\n",
    "        \n",
    "        return [img.transpose((2,0,1)).astype(np.double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:29.998902Z",
     "iopub.status.idle": "2022-04-17T18:36:29.999902Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.001403Z",
     "iopub.status.idle": "2022-04-17T18:36:30.002388Z"
    }
   },
   "outputs": [],
   "source": [
    "IN_KERNEL = os.environ.get('KAGGLE_WORKING_DIR') is not None\n",
    "MIN_SAMPLES_PER_CLASS = 30\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "MAX_STEPS_PER_EPOCH = 15000\n",
    "NUM_EPOCHS = 20\n",
    "LOG_FREQ = 400\n",
    "NUM_TOP_PREDICTS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.003954Z",
     "iopub.status.idle": "2022-04-17T18:36:30.004902Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/landmark-recognition-2020/train.csv')\n",
    "# test = pd.read_csv('../input/landmark-recognition-2020/sample_submission.csv')\n",
    "# train_dir = '../input/landmark-recognition-2020/train/'\n",
    "# test_dir = '../input/landmark-recognition-2020/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.006418Z",
     "iopub.status.idle": "2022-04-17T18:36:30.007437Z"
    }
   },
   "outputs": [],
   "source": [
    "# train, val = train_test_split(train, test_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.009033Z",
     "iopub.status.idle": "2022-04-17T18:36:30.010083Z"
    }
   },
   "outputs": [],
   "source": [
    "# class ImageDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataframe: pd.DataFrame, image_dir:str, mode: str):\n",
    "#         self.df = dataframe\n",
    "#         self.mode = mode\n",
    "#         self.image_dir = image_dir\n",
    "        \n",
    "#         transforms_list = []\n",
    "#         if self.mode == 'train':\n",
    "#             # Increase image size from (64,64) to higher resolution,\n",
    "#             # Make sure to change in RandomResizedCrop as well.\n",
    "#             transforms_list = [\n",
    "#                 transforms.Resize((64,64)),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.RandomChoice([\n",
    "#                     transforms.RandomResizedCrop(64),\n",
    "#                     transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "#                     transforms.RandomAffine(degrees=15, translate=(0.2, 0.2),\n",
    "#                                             scale=(0.8, 1.2), shear=15,\n",
    "#                                             resample=Image.BILINEAR)\n",
    "#                 ]),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                       std=[0.229, 0.224, 0.225]),\n",
    "#             ]\n",
    "#         else:\n",
    "#             transforms_list.extend([\n",
    "#                 # Keep this resize same as train\n",
    "#                 transforms.Resize((64,64)),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                       std=[0.229, 0.224, 0.225]),\n",
    "#             ])\n",
    "#         self.transforms = transforms.Compose(transforms_list)\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         image_id = self.df.iloc[index].id\n",
    "#         image_path = f\"{self.image_dir}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg\"\n",
    "#         image = Image.open(image_path)\n",
    "#         image = self.transforms(image)\n",
    "\n",
    "#         if self.mode == 'test':\n",
    "#             return {'image':image}\n",
    "#         else:\n",
    "#             return {'image':image, \n",
    "#                     'target':self.df.iloc[index].landmark_id}\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.011749Z",
     "iopub.status.idle": "2022-04-17T18:36:30.012761Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(train):\n",
    "    counts = train.turtle_id.value_counts()\n",
    "    print('train_df', train.shape)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train.turtle_id.values)\n",
    "    print('found classes', len(label_encoder.classes_))\n",
    "\n",
    "    train.turtle_id = label_encoder.transform(train.turtle_id)\n",
    "\n",
    "    train_dataset = TurtleDataset(train, transform=transforms.Compose([Rescale(256)]))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "    return train_loader, label_encoder, len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.014251Z",
     "iopub.status.idle": "2022-04-17T18:36:30.015331Z"
    }
   },
   "outputs": [],
   "source": [
    "def adam(parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "    if isinstance(betas, str):\n",
    "        betas = eval(betas)\n",
    "    return optim.Adam(parameters,\n",
    "                      lr=lr,\n",
    "                      betas=betas,\n",
    "                      eps=eps,\n",
    "                      weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.016996Z",
     "iopub.status.idle": "2022-04-17T18:36:30.017902Z"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    ''' Computes and stores the average and current value '''\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.019334Z",
     "iopub.status.idle": "2022-04-17T18:36:30.020373Z"
    }
   },
   "outputs": [],
   "source": [
    "def GAP(predicts: torch.Tensor, confs: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    ''' Simplified GAP@1 metric: only one prediction per sample is supported '''\n",
    "    assert len(predicts.shape) == 1\n",
    "    assert len(confs.shape) == 1\n",
    "    assert len(targets.shape) == 1\n",
    "    assert predicts.shape == confs.shape and confs.shape == targets.shape\n",
    "\n",
    "    _, indices = torch.sort(confs, descending=True)\n",
    "\n",
    "    confs = confs.cpu().numpy()\n",
    "    predicts = predicts[indices].cpu().numpy()\n",
    "    targets = targets[indices].cpu().numpy()\n",
    "\n",
    "    res, true_pos = 0.0, 0\n",
    "\n",
    "    for i, (c, p, t) in enumerate(zip(confs, predicts, targets)):\n",
    "        rel = int(p == t)\n",
    "        true_pos += rel\n",
    "\n",
    "        res += true_pos / (i + 1) * rel\n",
    "\n",
    "    res /= targets.shape[0] # FIXME: incorrect, not all test images depict landmarks\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.022073Z",
     "iopub.status.idle": "2022-04-17T18:36:30.023142Z"
    }
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, train, label=False):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        if train:\n",
    "            one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "            one_hot.scatter_(1, label.cuda().view(-1, 1).long(), 1)\n",
    "            output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        else:\n",
    "            output = cosine\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.024590Z",
     "iopub.status.idle": "2022-04-17T18:36:30.025745Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.027343Z",
     "iopub.status.idle": "2022-04-17T18:36:30.028368Z"
    }
   },
   "outputs": [],
   "source": [
    "class EfficientNetEncoderHead(nn.Module):\n",
    "    def __init__(self, depth, num_classes):\n",
    "        super(EfficientNetEncoderHead, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.base = efficientnet_pytorch.EfficientNet.from_pretrained(f'efficientnet-b{self.depth}')\n",
    "        self.gem = GeM()\n",
    "        self.output_filter = self.base._fc.in_features\n",
    "        self.fc = nn.Linear(self.output_filter, 1000)\n",
    "        self.arcface = ArcMarginProduct(1000, num_classes)\n",
    "    def forward(self, x, label):\n",
    "        x = self.base.extract_features(x)\n",
    "        x = self.gem(x).squeeze()\n",
    "        x = self.fc(x)\n",
    "        if self.training:\n",
    "            x = self.arcface(x, self.training, label)\n",
    "        else:\n",
    "            x = self.arcface(x, self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation\n",
    "This 'val_step' caliculate a validation accuracy and GAP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.029834Z",
     "iopub.status.idle": "2022-04-17T18:36:30.030829Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_step(val_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        label_encoder,\n",
    "        all_val_count):\n",
    "    \n",
    "    val_losses = AverageMeter()\n",
    "    val_gap_score = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    model.eval()\n",
    "    acc_count = 0\n",
    "    first = True\n",
    "    end = time.time()\n",
    "    for i, j in val_loader:\n",
    "        input_ = i\n",
    "        target = j\n",
    "        batch_size, _, _, _ = input_.shape\n",
    "        \n",
    "        output = model(input_.float().cuda(), target.cuda())\n",
    "        confs, predicts = torch.max(output.detach(), dim=1)\n",
    "        \n",
    "        if first:\n",
    "            all_confs = confs\n",
    "            all_predicts = predicts\n",
    "            all_targets = target\n",
    "            first = False\n",
    "        else:\n",
    "            all_confs = torch.cat([all_confs, confs])\n",
    "            all_predicts = torch.cat([all_predicts, predicts])\n",
    "            all_targets = torch.cat([all_targets, target])\n",
    "\n",
    "    val_gap_score = GAP(all_predicts, all_confs, all_targets)\n",
    "    val_gap_score = val_gap_score * len(all_confs) / all_val_count\n",
    "    \n",
    "    for i, (c, p, t) in enumerate(zip(all_confs, all_predicts, all_targets)):\n",
    "        if p == t:\n",
    "            acc_count += 1\n",
    "                \n",
    "    acc = float(acc_count) / all_val_count\n",
    "    val_time = time.time() - end\n",
    "    return acc, val_gap_score, val_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.032376Z",
     "iopub.status.idle": "2022-04-17T18:36:30.033459Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(train_loader, \n",
    "          model, \n",
    "          criterion, \n",
    "          optimizer,\n",
    "          epoch, \n",
    "          lr_scheduler):\n",
    "    print(f'epoch {epoch}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    avg_score = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    num_steps = min(len(train_loader), MAX_STEPS_PER_EPOCH)\n",
    "\n",
    "    print(f'total batches: {num_steps}')\n",
    "\n",
    "    end = time.time()\n",
    "    lr = None\n",
    "\n",
    "    for i, j in train_loader:\n",
    "        input_ = i\n",
    "        target =j\n",
    "        batch_size, _, _, _ = input_.shape\n",
    "        \n",
    "        output = model(input_.float().cuda(), target.cuda())\n",
    "        loss = criterion(output, target.cuda())\n",
    "        confs, predicts = torch.max(output.detach(), dim=1)\n",
    "        avg_score.update(GAP(predicts, confs, target))\n",
    "        losses.update(loss.data.item(), input_.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        \n",
    "    acc, val_gap, val_time = val_step(train_loader, model, criterion, label_encoder, len(dataset))\n",
    "    print('validation with training data '+str(val_time))\n",
    "    print(f'time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            f'loss {losses.val:.4f} ({losses.avg:.4f})\\t'\n",
    "            f'GAP {avg_score.val:.4f} ({avg_score.avg:.4f})\\t'\n",
    "            f'val_acc {acc}\\t'\n",
    "            f'val_GAP {val_gap:.4f}\\t'\n",
    "                 )\n",
    "            #slack = slackweb.Slack(url=\"~~~~~~\")\n",
    "            #slack.notify(text= f'{epoch} [{i}/{num_steps}]\\t'\n",
    "                    #f'time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    #f'loss {losses.val:.4f} ({losses.avg:.4f})\\t'\n",
    "                    #f'GAP {avg_score.val:.4f} ({avg_score.avg:.4f})\\t'\n",
    "                    #f'val_acc {acc}\\t'\n",
    "                    #f'val_GAP {val_gap:.4f}\\t')\n",
    "              \n",
    "\n",
    "    print(f' * average GAP on train {avg_score.avg:.4f}')\n",
    "    print(f' time {batch_time.sum:.4f}')\n",
    "    return avg_score.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.034961Z",
     "iopub.status.idle": "2022-04-17T18:36:30.036239Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(data_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    activation = nn.Softmax(dim=1)\n",
    "    all_predicts, all_confs, all_targets = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(data_loader, disable=IN_KERNEL)):\n",
    "            if data_loader.dataset.mode != 'test':\n",
    "                input_, target = data['image'], data['target']\n",
    "            else:\n",
    "                input_, target = data['image'], None\n",
    "\n",
    "            output = model(input_.cuda())\n",
    "            output = activation(output)\n",
    "\n",
    "            confs, predicts = torch.topk(output, NUM_TOP_PREDICTS)\n",
    "            all_confs.append(confs)\n",
    "            all_predicts.append(predicts)\n",
    "\n",
    "            if target is not None:\n",
    "                all_targets.append(target)\n",
    "\n",
    "    predicts = torch.cat(all_predicts)\n",
    "    confs = torch.cat(all_confs)\n",
    "    targets = torch.cat(all_targets) if len(all_targets) else None\n",
    "\n",
    "    return predicts, confs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, check if there is a trained model. If I have one, I will load it also with optimizer and scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made \"selected_classes.csv\". This file shows which classes are selected, in another word, this model is classifying into these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-17T11:38:08.340585Z",
     "iopub.status.busy": "2022-04-17T11:38:08.340257Z",
     "iopub.status.idle": "2022-04-17T16:06:56.746066Z",
     "shell.execute_reply": "2022-04-17T16:06:56.744778Z",
     "shell.execute_reply.started": "2022-04-17T11:38:08.340555Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    modelname = 'the_model'\n",
    "    input_dir = '../input/'\n",
    "    \n",
    "    global_start_time = time.time()\n",
    "    train_loader, label_encoder, num_classes = load_data(dataset)\n",
    "\n",
    "    all_classes = label_encoder.classes_\n",
    "    all_classes = list(all_classes)\n",
    "    selected_classes = train.turtle_id\n",
    "    with open('selected_classes.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(all_classes)\n",
    "    \n",
    "    model = EfficientNetEncoderHead(depth=7, num_classes=num_classes)\n",
    "    model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = adam(model.parameters(), lr=1e-3, betas=(0.9,0.999), eps=1e-3, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)*NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    s = False\n",
    "    # if there is 'learning.txt', read it and start training from the epoch which is written in that file.\n",
    "    if os.path.exists(input_dir + 'learning.txt'):\n",
    "        with open(input_dir + 'learning.txt') as f:\n",
    "            s = f.read()\n",
    "            \n",
    "    # opttimizer saving dir\n",
    "    opt_shc_path = 'optimizer_and_scheduler'\n",
    "        \n",
    "    \n",
    "    if s:\n",
    "        model.load_state_dict(torch.load(input_dir + 'the_model'+s+'.pth'))\n",
    "        start_epoch = int(s) + 1\n",
    "        checkpoint = torch.load(input_dir + 'optimizer_and_scheduler')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        print('optimizer and scheduler are loaded')\n",
    "        \n",
    "        pre_history = pd.read_csv(input_dir + 'the_model_history.csv')\n",
    "    else:\n",
    "        pre_history = pd.DataFrame(columns=['epoch', 'GAP', 'loss'])\n",
    "        start_epoch = 1\n",
    "\n",
    "        \n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        print('-' * 50)\n",
    "        score, loss = train_step(train_loader, model, criterion, optimizer, epoch, scheduler)\n",
    "        pre_history = pre_history.append({'GAP':score,'epoch':epoch,'loss':loss}, ignore_index=True)\n",
    "        \n",
    "        model_path = 'the_model'+str(epoch)+'.pth'\n",
    "        state = {\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save(state, opt_shc_path)\n",
    "        \n",
    "        with open('learning.txt', mode='w') as f:\n",
    "            f.write(str(epoch))\n",
    "            \n",
    "#         acc, val_gap, _ = val_step(val_loader, model, criterion, label_encoder, all_val_count)\n",
    "        \n",
    "        pre_history.to_csv('the_model_history.csv')\n",
    "            \n",
    "        # if you want to know about learning on slack\n",
    "        #slack = slackweb.Slack(url=\"~~~\")\n",
    "        #slack.notify(text= f'{epoch:.4f} epoch finished\\t'\n",
    "        #            f'val_acc {acc}\\t'\n",
    "        #            f'val_GAP {val_gap:.4f}\\t')\n",
    "        \n",
    "    #slack.notify(text= 'all learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.037900Z",
     "iopub.status.idle": "2022-04-17T18:36:30.038954Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_test(train):\n",
    "    print('train_df', train.shape)\n",
    "\n",
    "    train_dataset = TurtleDataset(train, transform=transforms.Compose([Rescale(256)]))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "    return train_loader\n",
    "\n",
    "\n",
    "test_loader = load_data_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.040470Z",
     "iopub.status.idle": "2022-04-17T18:36:30.041504Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_step(val_loader,\n",
    "        model):\n",
    "    \n",
    "    model.eval()\n",
    "    for i in val_loader:\n",
    "        input_ = i\n",
    "#         batch_size, _, _, _ = input_.shape\n",
    "        print(i)\n",
    "        \n",
    "        output = model(input_[0].float().cuda())\n",
    "        confs, predicts = torch.max(output.detach(), dim=1)\n",
    "        break\n",
    "        \n",
    "    print(predicts)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:36:30.043036Z",
     "iopub.status.idle": "2022-04-17T18:36:30.044077Z"
    }
   },
   "outputs": [],
   "source": [
    "test_step(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
